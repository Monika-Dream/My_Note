2022 / 10 / 8
----------------------------------------------------------------------------------------------------------------------
----------------------------------------------------------- 爬虫常用解析信息方式
result_data.text		返回出文本
result_data.json()		返回出 JSON 数据, 常常导入 import json 
result_data.content		常用于输出图片信息,返回二进制形式的图片数据
response.status_code	响应状态码
----------------------------------------------------------------------------------------------------------------------
----------------------------------------------------------- 爬虫常用请求头信息

 - User-Agent: 请求载体的身份标识
 - Connection: 请求完毕后, 是断开连接还是保持连接

----------------------------------------------------------- 爬虫常用响应头信息

 - Content-Type: 服务器响应回客户端的数据类型

----------------------------------------------------------- 网站安全加密方式

 - 对称密钥加密
 - 非对称密钥加密
 - 证书密钥加密		<- https 所采用方式

----------------------------------------------------------- requests 爬虫模块

 - 模拟浏览器发送请求
	 - 指定 url
	 - 发起请求	 get/post
	 - 获取响应数据
	 - 持久化存储
	 - 无法异步请求

----------------------------------------------------------- requests 爬虫模块爬取 qingju.ga 页面

import requests

if __name__ == '__main__':
    						# 指定 URL
    url = "https://qingju.ga/"
    						# requests 发起 Get 请求, 会返回一个响应对象
    response = requests.get(url=url)
    						# 查看响应的信息内容(源码数据)
    page_text = response.text
    with open("./Galgame网站源码.html","wb+") as file:
        file.write(page_text.encode())
    print("页面爬取结束")

----------------------------------------------------------- 实现简易网页采集器

 - UA伪装: 让爬虫对应的请求载体身份标识伪装为某一款浏览器

import requests

if __name__ == '__main__':
    					# TODO 修改 User-Agent 伪装为浏览器
    headers = {
        'User-Agent': "Mozilla / 5.0(Windows NT 10.0;Win64;x64) AppleWebKit / 537.36(KHTML, likeGecko) Chrome / 106.0.0.0Safari / 537.36Edg / 106.0.1370.37"
    }
    					# TODO 请求 URL
    url = "https://www.sogou.com/web"
    					# TODO 请求体
    search = input("请输入要搜索的内容")
    					# TODO 处理 URL 所带参数
    param = {
        "query": search
    }
    response = requests.get(url=url, params=param, headers=headers)
    result_data = response.text
    with open("./自定义网页请求.html","bw") as file:
        file.write(result_data.encode())

----------------------------------------------------------- 破解百度翻译

import requests
import json

if __name__ == '__main__':
    url = "https://fanyi.baidu.com/sug"
    user = {
        'User-Agent': "Mozilla / 5.0(Windows NT 10.0;Win64;x64) AppleWebKit / 537.36(KHTML, likeGecko) Chrome / 106.0.0.0Safari / 537.36Edg / 106.0.1370.37"
    }
    change_text = input("请输入要翻译的内容 中->英 \n")
    query = {
        "kw": change_text
    }
    response = requests.post(url=url,data=query)
    dic_obj = response.json()
    path = open("./dog.json","w",encoding="utf-8")
        #中文不能使用 ensure_ascii 编码, 所以要变 False
    json.dump(dic_obj,path,ensure_ascii=False)
    path.close()

----------------------------------------------------------------------------------------------------------------------
----------------------------------------------------------- 数据解析操作

 - 聚焦爬虫
 - 正则
 - bs4
 - xpath

----------------------------------------------------------------------------------------------------------------------
----------------------------------------------------------- bs4大致介绍

 - 是 Python 中独有的解析方式, 无法使用在其他语言之中
 - 解析原理
	 - 实例化一个 BeautifulSoup 中, 并将页面源码数据加载到该对象中
	 - 通过调用 BeautifulSoup 中相关的属性与方法进行标签定位和数据提取
 - 环境安装
	 - pip install bs4
	 - pip install lxml
 - 如何使用
	 - from bs4 import BeautifulSoup
	 - BeautifulSoup(html文档,"lxml")

----------------------------------------------------------------------------------------------------------------------
----------------------------------------------------------- bs4使用

 - 1.定位第一次在页面出现标签及标签内容一并返回			      .TagName
from bs4 import BeautifulSoup
soup = BeautifulSoup(html文档,"lxml")
print(soup.标签名称)				#定位出所有标签为[  ]的 


 - 2.定位第一次标签名为 [ ] 的标签及标签内容一并返回			      .find( )
	 - class_ 仅仅是因为关键字
from bs4 import BeautifulSoup
soup = BeautifulSoup(html文档,"lxml")
print(soup.find( 'a' ))			#定位出第一次标签为 a 的 
print(soup.find( 'a', class_=' song ' ))		#定位出第一次标签为 a 的, 且属性为class = 'song'


 - 3.查找所有标签名为 [ ] 的标签及标签内容一并返回( 列表 )		      .find_all( )
from bs4 import BeautifulSoup
soup = BeautifulSoup(html文档,"lxml")
print(soup.find_all( 'a' ))			#查找出所有标签为 a 的 
print(soup.find_all( 'a', class_=' song ' ))	#查找出第一次标签为 a 的, 且属性为class = 'song'


 - 4.使用选择器来定位出标签元素					      .select( )
from bs4 import BeautifulSoup
soup = BeautifulSoup(html文档,"lxml")
print(soup.select( '.TagName' ))		#使用选择器来定位出标签元素	

----------------------------------------------------------------------------------------------------------------------
----------------------------------------------------------- bs4 针对元素提取之后的一些针对提取函数

 - 这里将用 result_data 来代表 soup.find( 'a' ) 这些的带标签返回值	提取文本内容
1.result_data.text		获取标签中的所有内容
2.result_data.get_text()	获取标签中的所有内容
3.result_data.string		只可以获取该标签下面直系的标签内容( 不会去获取子集文本 )

 - 这里持续将用 result_data 来代表 soup.find( 'a' ) 这些的带标签返回值	提取属性内容
1.result_data.a[ 'href' ]	获取 a 标签中 href 属性内容

----------------------------------------------------------------------------------------------------------------------
----------------------------------------------------------- Xpath大致介绍

 - 实例化一个 etree 的对象, 且需要将需要解析的页面加载到该对象中
 - 调用 etree 对象中的 xpath 方法结合着 xpath 表达式实现标签的定位与内容的捕获
 - 需要安装环境 	pip install lxml
 - 拥有两种使用方法
	 - 本地解析
		form lxml import etree
		etree.parse(filePath)
	 - 网络解析
		form lxml import etree
		etree.HTML('page_text')

----------------------------------------------------------------------------------------------------------------------
----------------------------------------------------------- Xpath使用

 - / 标识下一个层级, 需要写出下一个层级名称
 - // 标识下一个层级, 也表示从任意位置找到

1.xpath 表达式依旧是依靠着层级关系进行定位
from lxml import etree
parser = etree.HTMLParser(encoding="UTF-8")
tree = etree.parse("test.html",parser=parser)
tree.xpath("/html/head/title")

2.xpath 属性选择写法
from lxml import etree
parser = etree.HTMLParser(encoding="UTF-8")
tree = etree.parse("test.html",parser=parser)
tree.xpath("/html/head/title[@class='song']")

3.xpath 索引选择写法		从一开始
from lxml import etree
parser = etree.HTMLParser(encoding="UTF-8")
tree = etree.parse("test.html",parser=parser)
tree.xpath("/html/head/title[@class='song']/a[3]")

4.多次选取
//div[@class='download-url']/a[1]/@href | //h1[@class='title']/text()
----------------------------------------------------------------------------------------------------------------------
----------------------------------------------------------- xpath 针对元素提取之后的一些针对提取函数

1.取文本内容
 - 直接在路径后面写 text() 即可, 返回列表, 如需获取非直系文本, 只需将 /text() 变 //text()
tree.xpath("/html/head/title[@class='song']/a[3]/text()")

2.取属性内容

tree.xpath("/html/head/title[@class='song']/a[3]/@href")

----------------------------------------------------------------------------------------------------------------------
----------------------------------------------------------- 超级🦅识别验证码图片方式

 - 打开工具包
账号 Monika
密码 sxhmzz954578
软件ID:		939969	
软件说明:		软件KEY:	ea37b44e02accbd58f11239f7d096658


----------------------------------------------------------------------------------------------------------------------
----------------------------------------------------------- 使用 ddddcor模块 识别验证码

 - pip install ddddocr
 - 请去相关文档查看使用方法

----------------------------------------------------------------------------------------------------------------------
----------------------------------------------------------- 关于拥有 Cookie 网站怎么模拟登陆

 - 使用 session 会话对象
	 - 1.可以进行请求的发送
	 - 2.如果请求过程中产生了 Cookie 值, 则该 Cookie 会被自动存储/携带在该 session 对象中
		 - 创建一个 session 对象
			 - session = requests.Session( )
		 - 使用 session 对象进行模拟登陆 post 请求的发送 (cookie 会被存储到 session 中)
		 - session 对象对个人主页对应的 get 请求进行发送 ( 携带了cookie )
 - 区别
	 - 1.requests 每次都是单独请求
	 - 2.session 可以储存上次请求的服务器　cookie

----------------------------------------------------------------------------------------------------------------------
----------------------------------------------------------- session 使用

session = requests.Session( )
session.get()
session.post()

----------------------------------------------------------------------------------------------------------------------
----------------------------------------------------------- 代理

 - 代理的作用
	 - 突破自身 IP 访问限制
	 - 隐藏自身的真实 IP
 - 代理相关的网站
	 - 快代理
	 - www.goubanjia.com

----------------------------------------------------------------------------------------------------------------------
----------------------------------------------------------- 异步爬虫

 - 多线程 / 多进程
	 - 好处: 单独为阻塞的操作单独开启线程或者进程, 阻塞操作就可以异步执行
	 - 坏处: 无法无限制的开启多线程或多进程
 - 线程池 / 进程池
	 - 好处: 可以降低系统对进程与线程销毁的频率, 降低系统的开销
	 - 坏处: 池中能开的拥有上限

----------------------------------------------------------------------------------------------------------------------
----------------------------------------------------------- 进程池异步爬虫( 以后请使用下面一课的方式 )

1.可迭代对象中需要放入 数组 之类的形似的类型 底层有 __item__ 的
2.使用上之后差不多跟 for 语句一样	

 - 导入进程池
from multiprocessing.dummy import Pool
 - 实例化类 与 创建多少个进程
pool = Pool( 4 )
 - 使用
pool.map(要执行的函数名, 可迭代对象)

----------------------------------------------------------------------------------------------------------------------
----------------------------------------------------------- 线程池异步爬虫
from concurrent.futures import ThreadPoolExecutor / ProcessPoolExecutor       线程池 / 进程池
with ThreadPoolExecutor(20) as t:
    for i in ["www.baidu.com",...]
        t.submit(函数名,MusicArr=i)        <- 传参方式

 - 一样的使用方法

----------------------------------------------------------------------------------------------------------------------
----------------------------------------------------------- 协程学习

 - 需要借助 asyncio 模块执行 协程对象
import asyncio

async def fun():			#返回协程对象
    print("Hello Monika")
    #time.sleep( 4 )			出现同步操作的时候, 异步就中断了	
    await asyncio.sleep(4)		#异步耗时操作, 需要挂起
    print("Hello Monika End")

async def fun1():
    print("Hello Sayori")
    await asyncio.sleep(2)
    print("Hello Sayori End")


async def main():
    tasks = [
       asyncio.create_task(fun()) ,	#需要包裹 asyncio.create_task()
       asyncio.create_task(fun1())
    ]
    await asyncio.wait(tasks)

if __name__ == '__main__':
        asyncio.run(main())


tasks = [  asyncio.create_task(download( url )) for url in urls  ]

----------------------------------------------------------------------------------------------------------------------
----------------------------------------------------------- aiohttp 读取方式简介( 解析信息方式 )

 - 数据二进制解析
response.content.read()             <==>    response.content

 - 数据文本化
response.text()                     <==>    response.text

 - 数据 JSON 化
response.json()                     <==>    response.json()

----------------------------------------------------------------------------------------------------------------------
----------------------------------------------------------- aiohttp 模块使用( 一次完整的单线程异步代码 )

import asyncio,aiohttp

urls = [
    "http://kr.shanghai-jiuxin.com/file/bizhi/20221017/fwfia1fs4ex.jpg",
    "http://kr.shanghai-jiuxin.com/file/bizhi/20221017/soszuvwqtai.jpg",
    "https://www.umei.cc/d/file/20221010/10ddc39143559f50acc19e4d90405f87.jpg"
]

async def download(url):
    name = url.rsplit("/",1)[1]
    print(name)
    print("正在请求",url)
    async with aiohttp.ClientSession(connector=aiohttp.TCPConnector(ssl=False)) as session:
        async with session.get(url=url) as response:
            with open(f"./协程爬取测试/{name}","wb") as file:
                file.write(await response.content.read())
    print("成功")

async def main():
    tasks = []
    for url in urls:
        tasks.append(asyncio.create_task(download(url)))
    await asyncio.wait(tasks)

if __name__ == '__main__':
    loop = asyncio.get_event_loop()
    loop.run_until_complete(main())

----------------------------------------------------------------------------------------------------------------------
----------------------------------------------------------- aiohttp 注意事项

1.报错: RuntimeError: Event loop is closed 
 - 使用   loop = asyncio.get_event_loop()   loop.run_until_complete(main()) 即可
 - 但是使用 asyncio.run(main()) 也行, 报错不影响结果
 - 目前查阅到是因为 asyncio.run() 自动提前关闭了 "任务队列", 上面没出错的原因是手动挡,还少了行 loop.close() 代码


2.报错: aiohttp.client_exceptions.ClientConnectorCertificateError: 
    Cannot connect to host www.umei.cc:443 ssl:True [SSLCertVerificationError: 
        (1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: certificate has expired (_ssl.c:1129)')]
 - 在 async with aiohttp.ClientSession() as session: 中添加 connector=aiohttp.TCPConnector(ssl=False) 即可
 - async with aiohttp.ClientSession(connector=aiohttp.TCPConnector(ssl=False)) as session:
 - 目前查阅的资料指向 https 证书的一系列问题

3.报错: "信号灯超时时间已到" 什么意思？
    async with aiohttp.ClientSession(connector=aiohttp.TCPConnector(ssl=False)) as session: 
    频繁创建, 注意复用

4.报错: aiohttp.client_exceptions.ServerDisconnectedError: Server disconnected





----------------------------------------------------------------------------------------------------------------------
----------------------------------------------------------- aiofiles 使用

 - 异步的写入文件
 async with aiofiles.open("路径",mode="w",encoding="utf-8") as file:
    await file.write(await response.content.read())

----------------------------------------------------------------------------------------------------------------------
----------------------------------------------------------- selenium 环境安装

 - 1.环境安装
 pip install selenium
 下载浏览器驱动 ( 注意对应版本 )
 http://chromedriver.storage.googleapis.com/index.html      <- 谷歌
 
----------------------------------------------------------------------------------------------------------------------
----------------------------------------------------------- selenium 使用
        
from selenium.webdriver import Chrome
from selenium.webdriver.common.keys import Keys             <- 键盘事件
from selenium.webdriver.support.select import Select        <- select 下拉菜单操作
from selenium
web = Chrome()
web.get("网址")                                             <- 打开网站


----------------------------------------------------------------------------------------------------------------------
----------------------------------------------------------- selenium 属性

 - 选中元素后的返回结果可以加 .text 输出
web.get("网址")                                  打开浏览器
web.close()                                     关闭当前窗口
web.title                                       网站名称
element = web.find_element(by='xpath',value='//*[@id="changeCityBox"]/p[1]/a')  查找属性
element.send_keys("内容",Keys.ENTER)             输入内容并且回车, 注意导入模块
element.click()                                 点击属性
web.implicitly_wait(3)                          等待
web.switch_to.window(web.window_handles[-1])    将 selenium 注意力切换到最后一个窗口
web.switch_to.default_content()                 回切到默认窗口

----------------------------------------------------------------------------------------------------------------------
----------------------------------------------------------- selenium 注意

1. - 在点击超链接到新增加的页面时, selenium 注意力还在上个窗口之中, 需要
     - web.switch_to.window(web.window_handles[-1])    将 selenium 注意力切换到最后一个窗口
 - 在 web.close() 当前窗口时, 同上
     - web.switch_to.window(web.window_handles[0])

2. - iframe 内容获取方式
     - 定位到元素
     - web.switch_to.frame(元素)

3. select 如何选择内容
     - 导入 from selenium.webdriver.support.select import Select 
     - 定位到下拉菜单
     - sel = Select(上面返回的元素)
        - sel.select_by_index()                 根据 索引               切换
        - sel.select_by_index()                 根据 Value值            切换
        - sel.select_by_visible_text()          根据 你所看到的标签内容  切换

4. 被检测到了怎么办(浏览器大于或等于 88 ,且针对谷歌浏览器的方案)
     - from selenium.webdriver.chrome.options import Options
     - opt = Options()
     - opt.add_argument("--disable-blink-features=AutomationControlled")
     - web = Chrome(options=opt)

----------------------------------------------------------------------------------------------------------------------
----------------------------------------------------------- selenium 设置无头浏览器

 - 就是让浏览器在后台运行
from selenium.webdriver.chrome.options import Options
opt = Options()
opt.add_argument("--headless")
opt.add_argument("--disable-gpu")
web = Chrome(opt)

----------------------------------------------------------------------------------------------------------------------
----------------------------------------------------------- selenium 处理图像信息

 - 定位元素
 img = web.find_element(by='xpath',value='//*[@id="changeCityBox"]/p[1]/a').screenshot_as_png
     - 返回图片字节,可以供 超级🦅 直接使用

 - 如果是需要点击的图片
     - 导包 from selenium.webdriver.common.action_chains import ActionChains      <- 事件链
     - ActionChains(web).move_to_element_with_offset(图片element,x,y).click().perform()

----------------------------------------------------------------------------------------------------------------------
----------------------------------------------------------- selenium 处理拖拽验证

 - 导包 from selenium.webdriver.common.action_chains import ActionChains
 - ActionChains(web).drag_and_drop_by_offset(元素element,x,y).perform()

----------------------------------------------------------------------------------------------------------------------
----------------------------------------------------------- scrapy 框架环境搭建

 - pip install -i https://pypi.tuna.tsinghua.edu.cn/simple 包名 ( 临时使用清华源 )
 - 环境配置
     - https://blog.csdn.net/Tom197/article/details/119549236

----------------------------------------------------------------------------------------------------------------------
----------------------------------------------------------- scrapy 创建工程目录
 - scrapy startproject 项目名称
 - cd 到工程目录中
 - scrapy genspider 名字 网址
 - 执行工程 scrapy crawl 名字       所使用的是上面的名称
 - 记得在 setting.py 中将 君子协议 设置为 False
     - LOG_LEVEL = "ERROR" 只输出错误内容

----------------------------------------------------------------------------------------------------------------------
----------------------------------------------------------- scrapy 代码分析

class FirstchangeSpider(scrapy.Spider):
    name = 'firstChange'                        项目名称, 爬虫源文件的唯一标识 scrapy crawl firstChange
    # allowed_domains = ['qingju.eu.org']         允许的域名, 只爬取当前域名下的图片之类的, 一般注释掉
    start_urls = ['http://qingju.eu.org/']      要爬取的网站, 可以设置多个

    def parse(self, response):                  上面 start_urls 设置多少个这里会调用多少次
        pass

----------------------------------------------------------------------------------------------------------------------
----------------------------------------------------------- scrapy 解析




























